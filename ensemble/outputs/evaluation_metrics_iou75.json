{
  "dataset_info": {
    "total_images": 60,
    "total_ground_truth_boxes": 482,
    "total_predicted_boxes": 507,
    "iou_threshold": 0.75
  },
  "per_class_metrics": {
    "date_of_reciept": {
      "precision": 0.8167,
      "recall": 0.8448,
      "f1_score": 0.8305,
      "average_iou": 0.8504,
      "true_positives": 49,
      "false_positives": 11,
      "false_negatives": 9,
      "total_ground_truth": 58,
      "total_predictions": 60
    },
    "gstin": {
      "precision": 0.7593,
      "recall": 0.7193,
      "f1_score": 0.7387,
      "average_iou": 0.8502,
      "true_positives": 41,
      "false_positives": 13,
      "false_negatives": 16,
      "total_ground_truth": 57,
      "total_predictions": 54
    },
    "invoice_no": {
      "precision": 0.7377,
      "recall": 0.7759,
      "f1_score": 0.7563,
      "average_iou": 0.8351,
      "true_positives": 45,
      "false_positives": 16,
      "false_negatives": 13,
      "total_ground_truth": 58,
      "total_predictions": 61
    },
    "mobile_no": {
      "precision": 0.4559,
      "recall": 0.5439,
      "f1_score": 0.496,
      "average_iou": 0.8406,
      "true_positives": 31,
      "false_positives": 37,
      "false_negatives": 26,
      "total_ground_truth": 57,
      "total_predictions": 68
    },
    "product_table": {
      "precision": 0.9155,
      "recall": 0.9559,
      "f1_score": 0.9353,
      "average_iou": 0.9072,
      "true_positives": 65,
      "false_positives": 6,
      "false_negatives": 3,
      "total_ground_truth": 68,
      "total_predictions": 71
    },
    "store_address": {
      "precision": 0.7656,
      "recall": 0.8448,
      "f1_score": 0.8033,
      "average_iou": 0.8728,
      "true_positives": 49,
      "false_positives": 15,
      "false_negatives": 9,
      "total_ground_truth": 58,
      "total_predictions": 64
    },
    "store_name": {
      "precision": 0.8676,
      "recall": 0.8676,
      "f1_score": 0.8676,
      "average_iou": 0.8714,
      "true_positives": 59,
      "false_positives": 9,
      "false_negatives": 9,
      "total_ground_truth": 68,
      "total_predictions": 68
    },
    "total_amount": {
      "precision": 0.8197,
      "recall": 0.8621,
      "f1_score": 0.8403,
      "average_iou": 0.8477,
      "true_positives": 50,
      "false_positives": 11,
      "false_negatives": 8,
      "total_ground_truth": 58,
      "total_predictions": 61
    }
  },
  "overall_metrics": {
    "precision": 0.7673,
    "recall": 0.8071,
    "f1_score": 0.7867,
    "macro_precision": 0.7672,
    "macro_recall": 0.8018,
    "macro_f1": 0.7835,
    "mean_iou": 0.8594,
    "detection_accuracy": 0.6483
  }
}