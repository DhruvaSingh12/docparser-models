{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1b562a",
   "metadata": {},
   "source": [
    "# Medical Bill Detection Model Training\n",
    "\n",
    "This notebook implements training of a YOLOv11 model for detecting various elements in medical bills, including:\n",
    "- Date of Receipt\n",
    "- GSTIN\n",
    "- Invoice Number\n",
    "- Mobile Number\n",
    "- Product Table\n",
    "- Store Address\n",
    "- Store Name\n",
    "- Total Amount\n",
    "\n",
    "The training will run for 100 epochs with comprehensive performance metrics and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5fbd4c",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Dependencies\n",
    "\n",
    "First, we'll import all necessary libraries and check GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0faedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set the working directory to the dataset folder\n",
    "os.chdir(os.path.dirname(os.path.abspath('__file__')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e94c96",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "Let's analyze our dataset configuration and structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e742c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset configuration\n",
    "with open('data.yaml', 'r') as file:\n",
    "    dataset_config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Dataset Configuration:\")\n",
    "print(f\"Number of classes: {dataset_config['nc']}\")\n",
    "print(\"\\nClasses:\")\n",
    "for idx, name in enumerate(dataset_config['names']):\n",
    "    print(f\"{idx}: {name}\")\n",
    "\n",
    "# Count number of images in each split\n",
    "def count_images(path):\n",
    "    return len([f for f in os.listdir(path) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "\n",
    "train_path = os.path.join('train', 'images')\n",
    "valid_path = os.path.join('valid', 'images')\n",
    "test_path = os.path.join('test', 'images')\n",
    "\n",
    "print(\"\\nDataset Split:\")\n",
    "print(f\"Training images: {count_images(train_path)}\")\n",
    "print(f\"Validation images: {count_images(valid_path)}\")\n",
    "print(f\"Test images: {count_images(test_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38654006",
   "metadata": {},
   "source": [
    "## 3. Configure and Train YOLO11 Model\n",
    "\n",
    "Now we'll set up the YOLO11n model and train it on our medical bill detection task for 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93982d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize YOLO model\n",
    "model = YOLO('yolo11n.pt')  # Load YOLOv11n pretrained model (will auto-download)\n",
    "\n",
    "# Get the current directory (dataset folder) and construct absolute path to data.yaml\n",
    "current_dir = os.getcwd()\n",
    "data_yaml_path = os.path.join(current_dir, 'data.yaml')\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Data YAML path: {data_yaml_path}\")\n",
    "print(f\"Data YAML exists: {os.path.exists(data_yaml_path)}\")\n",
    "print(f\"All results will be saved in: {os.path.join(current_dir, 'training_results')}\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=data_yaml_path,  # absolute path to data.yaml\n",
    "    epochs=100,        # number of epochs\n",
    "    imgsz=640,        # image size\n",
    "    batch=16,         # batch size\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',  # device to use (GPU or CPU)\n",
    "    workers=0,        # set to 0 for Windows compatibility (avoids multiprocessing issues)\n",
    "    patience=50,      # early stopping patience\n",
    "    project=current_dir,   # save to current dataset directory\n",
    "    name='training_results',  # experiment name\n",
    "    exist_ok=True     # overwrite existing experiment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ea9ab4",
   "metadata": {},
   "source": [
    "## 4. View Training Results\n",
    "\n",
    "Let's check the final training metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf17187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the results CSV\n",
    "results_csv_path = Path('runs/medical_bill_detection/results.csv')\n",
    "if results_csv_path.exists():\n",
    "    df = pd.read_csv(results_csv_path)\n",
    "    \n",
    "    # Get the last epoch (final results)\n",
    "    last_row = df.iloc[-1]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING COMPLETED - FINAL METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total Epochs Trained: {int(last_row['epoch'])}\")\n",
    "    print(f\"\\nFinal Performance Metrics:\")\n",
    "    print(f\"  Precision (B): {last_row['metrics/precision(B)']:.4f}\")\n",
    "    print(f\"  Recall (B):    {last_row['metrics/recall(B)']:.4f}\")\n",
    "    print(f\"  mAP50 (B):     {last_row['metrics/mAP50(B)']:.4f}\")\n",
    "    print(f\"  mAP50-95 (B):  {last_row['metrics/mAP50-95(B)']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFinal Training Losses:\")\n",
    "    print(f\"  Box Loss:  {last_row['train/box_loss']:.4f}\")\n",
    "    print(f\"  Class Loss: {last_row['train/cls_loss']:.4f}\")\n",
    "    print(f\"  DFL Loss:   {last_row['train/dfl_loss']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nFinal Validation Losses:\")\n",
    "    print(f\"  Box Loss:  {last_row['val/box_loss']:.4f}\")\n",
    "    print(f\"  Class Loss: {last_row['val/cls_loss']:.4f}\")\n",
    "    print(f\"  DFL Loss:   {last_row['val/dfl_loss']:.4f}\")\n",
    "    \n",
    "    # Find best epoch\n",
    "    best_epoch = df['metrics/mAP50(B)'].idxmax() + 1\n",
    "    best_map50 = df['metrics/mAP50(B)'].max()\n",
    "    print(f\"\\nBest Epoch: {best_epoch} (mAP50: {best_map50:.4f})\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"Results CSV not found. Training may not have completed yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85de1a",
   "metadata": {},
   "source": [
    "## 5. Visualize Training Progress\n",
    "\n",
    "Let's visualize how the model improved over the training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_csv_path.exists():\n",
    "    # Create a figure with subplots for different metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Training Progress Over Epochs', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: mAP Metrics\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(df['epoch'], df['metrics/mAP50(B)'], label='mAP50', linewidth=2, color='#2E86AB')\n",
    "    ax.plot(df['epoch'], df['metrics/mAP50-95(B)'], label='mAP50-95', linewidth=2, color='#A23B72')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('mAP Score', fontsize=12)\n",
    "    ax.set_title('Mean Average Precision (mAP)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Precision and Recall\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(df['epoch'], df['metrics/precision(B)'], label='Precision', linewidth=2, color='#06A77D')\n",
    "    ax.plot(df['epoch'], df['metrics/recall(B)'], label='Recall', linewidth=2, color='#F18F01')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Precision & Recall', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Training Losses\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(df['epoch'], df['train/box_loss'], label='Box Loss', linewidth=2, color='#E63946')\n",
    "    ax.plot(df['epoch'], df['train/cls_loss'], label='Class Loss', linewidth=2, color='#457B9D')\n",
    "    ax.plot(df['epoch'], df['train/dfl_loss'], label='DFL Loss', linewidth=2, color='#2A9D8F')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Training Losses', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Validation Losses\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(df['epoch'], df['val/box_loss'], label='Val Box Loss', linewidth=2, color='#E63946')\n",
    "    ax.plot(df['epoch'], df['val/cls_loss'], label='Val Class Loss', linewidth=2, color='#457B9D')\n",
    "    ax.plot(df['epoch'], df['val/dfl_loss'], label='Val DFL Loss', linewidth=2, color='#2A9D8F')\n",
    "    ax.set_xlabel('Epoch', fontsize=12)\n",
    "    ax.set_ylabel('Loss', fontsize=12)\n",
    "    ax.set_title('Validation Losses', fontsize=14, fontweight='bold')\n",
    "    ax.legend(fontsize=10)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Training progress visualizations displayed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eacd7d",
   "metadata": {},
   "source": [
    "## 6. Validate Model Performance on Validation Set\n",
    "\n",
    "Let's load the best trained model and evaluate it on the validation set to get detailed per-class metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e54cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "best_model_path = Path('runs/medical_bill_detection/weights/best.pt')\n",
    "\n",
    "if best_model_path.exists():\n",
    "    print(\"Loading best model for validation...\")\n",
    "    best_model = YOLO(str(best_model_path))\n",
    "    \n",
    "    # Run validation\n",
    "    print(\"\\nRunning validation on validation set...\")\n",
    "    val_results = best_model.val(\n",
    "        data=data_yaml_path,\n",
    "        split='val',\n",
    "        batch=16,\n",
    "        imgsz=640,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"VALIDATION RESULTS ON VALIDATION SET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"  Box Precision:    {val_results.box.p.mean():.4f}\")\n",
    "    print(f\"  Box Recall:       {val_results.box.r.mean():.4f}\")\n",
    "    print(f\"  mAP50:            {val_results.box.map50:.4f}\")\n",
    "    print(f\"  mAP50-95:         {val_results.box.map:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\nPer-Class Metrics:\")\n",
    "    print(f\"{'Class':<20} {'Precision':<12} {'Recall':<12} {'mAP50':<12} {'mAP50-95':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    class_names = dataset_config['names']\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        if idx < len(val_results.box.ap_class_index):\n",
    "            precision = val_results.box.p[idx] if idx < len(val_results.box.p) else 0\n",
    "            recall = val_results.box.r[idx] if idx < len(val_results.box.r) else 0\n",
    "            map50 = val_results.box.ap50[idx] if idx < len(val_results.box.ap50) else 0\n",
    "            map = val_results.box.ap[idx] if idx < len(val_results.box.ap) else 0\n",
    "            \n",
    "            print(f\"{class_name:<20} {precision:<12.4f} {recall:<12.4f} {map50:<12.4f} {map:<12.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save validation metrics to CSV\n",
    "    val_metrics_data = {\n",
    "        'class': class_names,\n",
    "        'precision': [val_results.box.p[i] if i < len(val_results.box.p) else 0 for i in range(len(class_names))],\n",
    "        'recall': [val_results.box.r[i] if i < len(val_results.box.r) else 0 for i in range(len(class_names))],\n",
    "        'mAP50': [val_results.box.ap50[i] if i < len(val_results.box.ap50) else 0 for i in range(len(class_names))],\n",
    "        'mAP50-95': [val_results.box.ap[i] if i < len(val_results.box.ap) else 0 for i in range(len(class_names))]\n",
    "    }\n",
    "    \n",
    "    val_metrics_df = pd.DataFrame(val_metrics_data)\n",
    "    val_metrics_csv = results_dir / 'validation_metrics.csv'\n",
    "    val_metrics_df.to_csv(val_metrics_csv, index=False)\n",
    "    print(f\"\\nValidation metrics saved to: {val_metrics_csv}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98cc86b",
   "metadata": {},
   "source": [
    "## 7. Test Model Performance on Test Set\n",
    "\n",
    "Now let's evaluate the model on the held-out test set to measure real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dae820",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_path.exists():\n",
    "    print(\"Running evaluation on test set...\")\n",
    "    \n",
    "    # Run validation on test split\n",
    "    test_results = best_model.val(\n",
    "        data=data_yaml_path,\n",
    "        split='test',\n",
    "        batch=16,\n",
    "        imgsz=640,\n",
    "        device=0 if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TEST RESULTS ON TEST SET (UNSEEN DATA)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Overall metrics\n",
    "    print(\"\\nOverall Metrics:\")\n",
    "    print(f\"  Box Precision:    {test_results.box.p.mean():.4f}\")\n",
    "    print(f\"  Box Recall:       {test_results.box.r.mean():.4f}\")\n",
    "    print(f\"  mAP50:            {test_results.box.map50:.4f}\")\n",
    "    print(f\"  mAP50-95:         {test_results.box.map:.4f}\")\n",
    "    \n",
    "    # Per-class metrics\n",
    "    print(\"\\nPer-Class Test Metrics:\")\n",
    "    print(f\"{'Class':<20} {'Precision':<12} {'Recall':<12} {'mAP50':<12} {'mAP50-95':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        if idx < len(test_results.box.ap_class_index):\n",
    "            precision = test_results.box.p[idx] if idx < len(test_results.box.p) else 0\n",
    "            recall = test_results.box.r[idx] if idx < len(test_results.box.r) else 0\n",
    "            map50 = test_results.box.ap50[idx] if idx < len(test_results.box.ap50) else 0\n",
    "            map = test_results.box.ap[idx] if idx < len(test_results.box.ap) else 0\n",
    "            \n",
    "            print(f\"{class_name:<20} {precision:<12.4f} {recall:<12.4f} {map50:<12.4f} {map:<12.4f}\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Save test metrics to CSV\n",
    "    test_metrics_data = {\n",
    "        'class': class_names,\n",
    "        'precision': [test_results.box.p[i] if i < len(test_results.box.p) else 0 for i in range(len(class_names))],\n",
    "        'recall': [test_results.box.r[i] if i < len(test_results.box.r) else 0 for i in range(len(class_names))],\n",
    "        'mAP50': [test_results.box.ap50[i] if i < len(test_results.box.ap50) else 0 for i in range(len(class_names))],\n",
    "        'mAP50-95': [test_results.box.ap[i] if i < len(test_results.box.ap) else 0 for i in range(len(class_names))]\n",
    "    }\n",
    "    \n",
    "    test_metrics_df = pd.DataFrame(test_metrics_data)\n",
    "    test_metrics_csv = results_dir / 'test_metrics.csv'\n",
    "    test_metrics_df.to_csv(test_metrics_csv, index=False)\n",
    "    print(f\"\\nTest metrics saved to: {test_metrics_csv}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Best model not found at {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65625d6d",
   "metadata": {},
   "source": [
    "## 9. Compare Validation vs Test Performance\n",
    "\n",
    "Let's compare how the model performs on validation vs test set to check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a82878",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_path.exists():\n",
    "    # Create comparison dataframe\n",
    "    comparison_data = {\n",
    "        'Class': class_names,\n",
    "        'Val_Precision': [val_results.box.p[i] if i < len(val_results.box.p) else 0 for i in range(len(class_names))],\n",
    "        'Test_Precision': [test_results.box.p[i] if i < len(test_results.box.p) else 0 for i in range(len(class_names))],\n",
    "        'Val_Recall': [val_results.box.r[i] if i < len(val_results.box.r) else 0 for i in range(len(class_names))],\n",
    "        'Test_Recall': [test_results.box.r[i] if i < len(test_results.box.r) else 0 for i in range(len(class_names))],\n",
    "        'Val_mAP50': [val_results.box.ap50[i] if i < len(val_results.box.ap50) else 0 for i in range(len(class_names))],\n",
    "        'Test_mAP50': [test_results.box.ap50[i] if i < len(test_results.box.ap50) else 0 for i in range(len(class_names))],\n",
    "    }\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    fig.suptitle('Validation vs Test Performance Comparison', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('Precision', 'Val_Precision', 'Test_Precision'),\n",
    "        ('Recall', 'Val_Recall', 'Test_Recall'),\n",
    "        ('mAP50', 'Val_mAP50', 'Test_mAP50')\n",
    "    ]\n",
    "    \n",
    "    x = np.arange(len(class_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    for idx, (metric_name, val_col, test_col) in enumerate(metrics_to_plot):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        val_bars = ax.bar(x - width/2, comparison_df[val_col], width, label='Validation', \n",
    "                          color='#2E86AB', alpha=0.8)\n",
    "        test_bars = ax.bar(x + width/2, comparison_df[test_col], width, label='Test', \n",
    "                           color='#06A77D', alpha=0.8)\n",
    "        \n",
    "        ax.set_xlabel('Class', fontsize=11)\n",
    "        ax.set_ylabel(metric_name, fontsize=11)\n",
    "        ax.set_title(f'{metric_name} Comparison', fontsize=13, fontweight='bold')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(class_names, rotation=45, ha='right', fontsize=9)\n",
    "        ax.legend(fontsize=10)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_ylim([0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display comparison table\n",
    "    print(\"\\n\" + \"=\"*120)\n",
    "    print(\"VALIDATION vs TEST PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*120)\n",
    "    print(comparison_df.to_string(index=False))\n",
    "    print(\"=\"*120)\n",
    "    \n",
    "    # Calculate and display overall comparison\n",
    "    print(\"\\nOverall Performance Summary:\")\n",
    "    print(f\"{'Metric':<20} {'Validation':<15} {'Test':<15} {'Difference':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    metrics_summary = [\n",
    "        ('Precision', val_results.box.p.mean(), test_results.box.p.mean()),\n",
    "        ('Recall', val_results.box.r.mean(), test_results.box.r.mean()),\n",
    "        ('mAP50', val_results.box.map50, test_results.box.map50),\n",
    "        ('mAP50-95', val_results.box.map, test_results.box.map)\n",
    "    ]\n",
    "    \n",
    "    for metric_name, val_score, test_score in metrics_summary:\n",
    "        diff = test_score - val_score\n",
    "        diff_str = f\"{diff:+.4f}\"\n",
    "        print(f\"{metric_name:<20} {val_score:<15.4f} {test_score:<15.4f} {diff_str:<15}\")\n",
    "    \n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    # Save comparison to CSV\n",
    "    comparison_csv = results_dir / 'validation_vs_test_comparison.csv'\n",
    "    comparison_df.to_csv(comparison_csv, index=False)\n",
    "    print(f\"\\nComparison metrics saved to: {comparison_csv}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Cannot perform comparison without validation and test results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e07baf9",
   "metadata": {},
   "source": [
    "## 10. Generate Comprehensive Model Report\n",
    "\n",
    "Create a final summary report with all key information about the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "if best_model_path.exists():\n",
    "    # Create comprehensive report\n",
    "    report = {\n",
    "        'model_info': {\n",
    "            'model_type': 'YOLOv11n',\n",
    "            'task': 'Medical Bill Detection',\n",
    "            'num_classes': dataset_config['nc'],\n",
    "            'class_names': dataset_config['names'],\n",
    "            'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'best_model_path': str(best_model_path),\n",
    "            'last_model_path': str(best_model_path.parent / 'last.pt')\n",
    "        },\n",
    "        'training_config': {\n",
    "            'epochs': int(last_row['epoch']),\n",
    "            'image_size': 640,\n",
    "            'batch_size': 16,\n",
    "            'device': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
    "            'patience': 50\n",
    "        },\n",
    "        'dataset_info': {\n",
    "            'train_images': count_images(train_path),\n",
    "            'validation_images': count_images(valid_path),\n",
    "            'test_images': count_images(test_path),\n",
    "            'total_images': count_images(train_path) + count_images(valid_path) + count_images(test_path)\n",
    "        },\n",
    "        'final_training_metrics': {\n",
    "            'precision': float(last_row['metrics/precision(B)']),\n",
    "            'recall': float(last_row['metrics/recall(B)']),\n",
    "            'mAP50': float(last_row['metrics/mAP50(B)']),\n",
    "            'mAP50-95': float(last_row['metrics/mAP50-95(B)']),\n",
    "            'box_loss': float(last_row['train/box_loss']),\n",
    "            'class_loss': float(last_row['train/cls_loss']),\n",
    "            'dfl_loss': float(last_row['train/dfl_loss'])\n",
    "        },\n",
    "        'validation_metrics': {\n",
    "            'overall': {\n",
    "                'precision': float(val_results.box.p.mean()),\n",
    "                'recall': float(val_results.box.r.mean()),\n",
    "                'mAP50': float(val_results.box.map50),\n",
    "                'mAP50-95': float(val_results.box.map)\n",
    "            },\n",
    "            'per_class': {}\n",
    "        },\n",
    "        'test_metrics': {\n",
    "            'overall': {\n",
    "                'precision': float(test_results.box.p.mean()),\n",
    "                'recall': float(test_results.box.r.mean()),\n",
    "                'mAP50': float(test_results.box.map50),\n",
    "                'mAP50-95': float(test_results.box.map)\n",
    "            },\n",
    "            'per_class': {}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add per-class metrics\n",
    "    for idx, class_name in enumerate(class_names):\n",
    "        if idx < len(val_results.box.ap_class_index):\n",
    "            report['validation_metrics']['per_class'][class_name] = {\n",
    "                'precision': float(val_results.box.p[idx]) if idx < len(val_results.box.p) else 0,\n",
    "                'recall': float(val_results.box.r[idx]) if idx < len(val_results.box.r) else 0,\n",
    "                'mAP50': float(val_results.box.ap50[idx]) if idx < len(val_results.box.ap50) else 0,\n",
    "                'mAP50-95': float(val_results.box.ap[idx]) if idx < len(val_results.box.ap) else 0\n",
    "            }\n",
    "            \n",
    "        if idx < len(test_results.box.ap_class_index):\n",
    "            report['test_metrics']['per_class'][class_name] = {\n",
    "                'precision': float(test_results.box.p[idx]) if idx < len(test_results.box.p) else 0,\n",
    "                'recall': float(test_results.box.r[idx]) if idx < len(test_results.box.r) else 0,\n",
    "                'mAP50': float(test_results.box.ap50[idx]) if idx < len(test_results.box.ap50) else 0,\n",
    "                'mAP50-95': float(test_results.box.ap[idx]) if idx < len(test_results.box.ap) else 0\n",
    "            }\n",
    "    \n",
    "    # Save report as JSON\n",
    "    report_json_path = results_dir / 'model_report.json'\n",
    "    with open(report_json_path, 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    # Print formatted report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" \" * 25 + \"MODEL TRAINING REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nMODEL INFORMATION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Model Type:           {report['model_info']['model_type']}\")\n",
    "    print(f\"  Task:                 {report['model_info']['task']}\")\n",
    "    print(f\"  Number of Classes:    {report['model_info']['num_classes']}\")\n",
    "    print(f\"  Training Date:        {report['model_info']['training_date']}\")\n",
    "    \n",
    "    print(\"\\nTRAINING CONFIGURATION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Epochs:               {report['training_config']['epochs']}\")\n",
    "    print(f\"  Image Size:           {report['training_config']['image_size']}\")\n",
    "    print(f\"  Batch Size:           {report['training_config']['batch_size']}\")\n",
    "    print(f\"  Device:               {report['training_config']['device']}\")\n",
    "    \n",
    "    print(\"\\nDATASET INFORMATION\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Training Images:      {report['dataset_info']['train_images']}\")\n",
    "    print(f\"  Validation Images:    {report['dataset_info']['validation_images']}\")\n",
    "    print(f\"  Test Images:          {report['dataset_info']['test_images']}\")\n",
    "    print(f\"  Total Images:         {report['dataset_info']['total_images']}\")\n",
    "    \n",
    "    print(\"\\nFINAL PERFORMANCE METRICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Dataset':<15} {'Precision':<12} {'Recall':<12} {'mAP50':<12} {'mAP50-95':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Validation':<15} {report['validation_metrics']['overall']['precision']:<12.4f} \"\n",
    "          f\"{report['validation_metrics']['overall']['recall']:<12.4f} \"\n",
    "          f\"{report['validation_metrics']['overall']['mAP50']:<12.4f} \"\n",
    "          f\"{report['validation_metrics']['overall']['mAP50-95']:<12.4f}\")\n",
    "    print(f\"{'Test':<15} {report['test_metrics']['overall']['precision']:<12.4f} \"\n",
    "          f\"{report['test_metrics']['overall']['recall']:<12.4f} \"\n",
    "          f\"{report['test_metrics']['overall']['mAP50']:<12.4f} \"\n",
    "          f\"{report['test_metrics']['overall']['mAP50-95']:<12.4f}\")\n",
    "    \n",
    "    print(\"\\nSAVED FILES\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Best Model:           {report['model_info']['best_model_path']}\")\n",
    "    print(f\"  Training Results:     {results_csv_path}\")\n",
    "    print(f\"  Validation Metrics:   {val_metrics_csv}\")\n",
    "    print(f\"  Test Metrics:         {test_metrics_csv}\")\n",
    "    print(f\"  Comparison:           {comparison_csv}\")\n",
    "    print(f\"  Model Report (JSON):  {report_json_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING, VALIDATION, AND TESTING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nComprehensive model report saved to: {report_json_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot generate report without model and results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
